{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternative Tracing Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far in this module, we've taken a look at the traceable decorator, and how we can use it to set up tracing.\n",
    "\n",
    "In this lesson, we're going to look at alternative ways in which we can set up tracing, and when you should think about using these different approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain and LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are using LangChain or LangGraph, all we need to do to set up tracing is to set a few environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can set them inline\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\"  # If you don't set this, traces will go to the Default project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or you can use a .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry too much about our graph implementation here, you can learn more about LangGraph through our LangGraph Academy course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=os.getenv(\"AZURE_OPENAI_DEPLOYMENT\"),  # or your deployment\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),  # or your api version\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")\n",
    "\n",
    "from langchain_azure_ai.chat_models import AzureAIChatCompletionsModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "# Initialize the model using Azure AI Foundry project details\n",
    "# Ensure your endpoint URL ends at '/openai/v1' or the base project root\n",
    "# llm = AzureAIChatCompletionsModel(\n",
    "#     endpoint=os.getenv(\"AZURE_AI_ENDPOINT\"),\n",
    "#     credential=os.getenv(\"AZURE_AI_CREDENTIAL\"),\n",
    "#     model_name=\"gpt-4o-mini\",\n",
    "#     api_version=\"2024-12-01-preview\",  # Or other models like \"Mistral-Large-3\"\n",
    "#     temperature=0.7,    \n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import operator\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import HumanMessage, AnyMessage, get_buffer_string\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "from utils import get_vector_db_retriever, RAG_PROMPT\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "retriever = get_vector_db_retriever()\n",
    "#llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Define Graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    documents: List[Document]\n",
    "\n",
    "# Define Nodes\n",
    "def retrieve_documents(state: GraphState):\n",
    "    messages = state.get(\"messages\", [])\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(f\"{get_buffer_string(messages)} {question}\")\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "def generate_response(state: GraphState):\n",
    "    question = state[\"question\"]\n",
    "    messages = state[\"messages\"]\n",
    "    documents = state[\"documents\"]\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    \n",
    "    rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n",
    "    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n",
    "    return {\"documents\": documents, \"messages\": [HumanMessage(question), generation]}\n",
    "\n",
    "# Define Graph\n",
    "graph_builder = StateGraph(GraphState)\n",
    "graph_builder.add_node(\"retrieve_documents\", retrieve_documents)\n",
    "graph_builder.add_node(\"generate_response\", generate_response)\n",
    "graph_builder.add_edge(START, \"retrieve_documents\")\n",
    "graph_builder.add_edge(\"retrieve_documents\", \"generate_response\")\n",
    "graph_builder.add_edge(\"generate_response\", END)\n",
    "\n",
    "simple_rag_graph = graph_builder.compile()\n",
    "display(Image(simple_rag_graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're setting up a simple graph in LangGraph. If you want to learn more about LangGraph, I would highly recommend taking a look at our LangGraph Academy course.\n",
    "\n",
    "You can also pass in metadata or other fields through an optional config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "(BadRequest) API version not supported\nCode: BadRequest\nMessage: API version not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHttpResponseError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m question = \u001b[33m\"\u001b[39m\u001b[33mHow do I set up tracing if I\u001b[39m\u001b[33m'\u001b[39m\u001b[33mm using LangChain?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msimple_rag_graph\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfoo\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbar\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:3094\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3091\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3092\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3094\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3095\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3096\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3097\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3098\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3099\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3100\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3102\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3103\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3104\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3106\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3108\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3109\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mgenerate_response\u001b[39m\u001b[34m(state)\u001b[39m\n\u001b[32m     34\u001b[39m formatted_docs = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(doc.page_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents)\n\u001b[32m     36\u001b[39m rag_prompt_formatted = RAG_PROMPT.format(context=formatted_docs, conversation=messages, question=question)\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m generation = \u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrag_prompt_formatted\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m\"\u001b[39m\u001b[33mdocuments\u001b[39m\u001b[33m\"\u001b[39m: documents, \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [HumanMessage(question), generation]}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:379\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    365\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    366\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    367\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    372\u001b[39m     **kwargs: Any,\n\u001b[32m    373\u001b[39m ) -> AIMessage:\n\u001b[32m    374\u001b[39m     config = ensure_config(config)\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    376\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    377\u001b[39m         cast(\n\u001b[32m    378\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    380\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    381\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    384\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    385\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    386\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    387\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    388\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    389\u001b[39m         ).message,\n\u001b[32m    390\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1088\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1079\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1081\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1085\u001b[39m     **kwargs: Any,\n\u001b[32m   1086\u001b[39m ) -> LLMResult:\n\u001b[32m   1087\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1088\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:903\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    900\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    901\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    902\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m903\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    904\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    905\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    909\u001b[39m         )\n\u001b[32m    910\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    911\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:1192\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1190\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1196\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langchain_azure_ai/chat_models/inference.py:546\u001b[39m, in \u001b[36mAzureAIChatCompletionsModel._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    538\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_generate\u001b[39m(\n\u001b[32m    539\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    540\u001b[39m     messages: List[BaseMessage],\n\u001b[32m   (...)\u001b[39m\u001b[32m    543\u001b[39m     **kwargs: Any,\n\u001b[32m    544\u001b[39m ) -> ChatResult:\n\u001b[32m    545\u001b[39m     inference_messages = to_inference_message(messages)\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcomplete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43minference_messages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    548\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    549\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_identifying_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    550\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    551\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    552\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/azure/ai/inference/_patch.py:738\u001b[39m, in \u001b[36mChatCompletionsClient.complete\u001b[39m\u001b[34m(self, body, messages, stream, frequency_penalty, presence_penalty, temperature, top_p, max_tokens, response_format, stop, tools, tool_choice, seed, model, model_extras, **kwargs)\u001b[39m\n\u001b[32m    736\u001b[39m         response.read()  \u001b[38;5;66;03m# Load the body in memory and close the socket\u001b[39;00m\n\u001b[32m    737\u001b[39m     map_error(status_code=response.status_code, response=response, error_map=error_map)\n\u001b[32m--> \u001b[39m\u001b[32m738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response=response)\n\u001b[32m    740\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _stream:\n\u001b[32m    741\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _models.StreamingChatCompletions(response)\n",
      "\u001b[31mHttpResponseError\u001b[39m: (BadRequest) API version not supported\nCode: BadRequest\nMessage: API version not supported",
      "During task with name 'generate_response' and id 'ec37d7ae-0374-941c-57a1-3ad72aaece74'"
     ]
    }
   ],
   "source": [
    "question = \"How do I set up tracing if I'm using LangChain?\"\n",
    "simple_rag_graph.invoke({\"question\": question}, config={\"metadata\": {\"foo\": \"bar\"}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Let's take a look in LangSmith!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracing Context Manager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, you can use the trace context manager to log traces to LangSmith. This is useful in situations where:\n",
    "\n",
    "You want to log traces for a specific block of code.\n",
    "You want control over the inputs, outputs, and other attributes of the trace.\n",
    "It is not feasible to use a decorator or wrapper.\n",
    "Any or all of the above.\n",
    "The context manager integrates seamlessly with the traceable decorator and wrap_openai wrapper, so you can use them together in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import traceable, trace\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "#openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "\"\"\"\n",
    "retrieve_documents\n",
    "- Returns documents fetched from a vectorstore based on the user's question\n",
    "\"\"\"\n",
    "@traceable\n",
    "def retrieve_documents(question: str):\n",
    "    documents = retriever.invoke(question)\n",
    "    return documents\n",
    "\n",
    "\"\"\"\n",
    "generate_response\n",
    "- Calls `call_openai` to generate a model response after formatting inputs\n",
    "\"\"\"\n",
    "# TODO: Remove traceable, and use with trace()\n",
    "@traceable\n",
    "def generate_response(question: str, documents):\n",
    "    # NOTE: Our documents came in as a list of objects, but we just want to log a string\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "\n",
    "    # TODO: Use with trace()\n",
    "    # with trace(\n",
    "    #     name=\"Generate Response\",\n",
    "    #     run_type=\"chain\", \n",
    "    #     inputs={\"question\": question, \"formatted_docs\": formatted_docs},\n",
    "    #     metadata={\"foo\": \"bar\"},\n",
    "    # ) as ls_trace:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    response = call_openai(messages)\n",
    "    # TODO: End your trace and write outputs to LangSmith\n",
    "    # ls_trace.end(outputs={\"output\": response})\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "call_openai\n",
    "- Returns the chat completion output from OpenAI\n",
    "\"\"\"\n",
    "@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict], model: str = MODEL_NAME, temperature: float = 0.0\n",
    ") -> str:\n",
    "    response = llm.invoke(messages)\n",
    "    #response = llm.chat.completions.create(\n",
    "    #    model=model,\n",
    "    #    messages=messages,\n",
    "    #    temperature=temperature,\n",
    "    #)\n",
    "    return response\n",
    "\n",
    "\"\"\"\n",
    "langsmith_rag\n",
    "- Calls `retrieve_documents` to fetch documents\n",
    "- Calls `generate_response` to generate a response based on the fetched documents\n",
    "- Returns the model response\n",
    "\"\"\"\n",
    "@traceable\n",
    "def langsmith_rag(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='To trace with tracing context, you need to configure tracing by setting up basic options or using framework integrations that support tracing context. This involves passing a unique identifier, such as session_id or thread_id, to link traces together within a thread. For detailed steps, refer to the tracing setup section in the LangSmith documentation.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 1438, 'total_tokens': 1503, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 1408}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f97eff32c5', 'id': 'chatcmpl-D6c1aJGtymCC7fQlx3lOPs7XogiFQ', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'jailbreak': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'finish_reason': 'stop', 'logprobs': None, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'protected_material_code': {'filtered': False, 'detected': False}, 'protected_material_text': {'filtered': False, 'detected': False}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}} id='lc_run--7210b0ff-1e84-4fbd-8100-472a66841165-0' usage_metadata={'input_tokens': 1438, 'output_tokens': 65, 'total_tokens': 1503, 'input_token_details': {'audio': 0, 'cache_read': 1408}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with tracing context?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wrap_openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrap_openai/wrapOpenAI methods in Python/TypeScript allow you to wrap your OpenAI client in order to automatically log traces -- no decorator or function wrapping required! The wrapper works seamlessly with the @traceable decorator or traceable function and you can use both in the same application.\n",
    "\n",
    "You still need to set your `LANGSMITH_API_KEY` and `LANGSMITH_TRACING`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Completions' object has no attribute 'chat'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     11\u001b[39m RAG_SYSTEM_PROMPT = \u001b[33m\"\"\"\u001b[39m\u001b[33mYou are an assistant for question-answering tasks. \u001b[39m\n\u001b[32m     12\u001b[39m \u001b[33mUse the following pieces of retrieved context to answer the latest question in the conversation. \u001b[39m\n\u001b[32m     13\u001b[39m \u001b[33mIf you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know the answer, just say that you don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt know. \u001b[39m\n\u001b[32m     14\u001b[39m \u001b[33mUse three sentences maximum and keep the answer concise.\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[33m\"\"\"\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# TODO: Wrap the OpenAI Client\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#openai_client = wrap_openai(openai.Client())\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#openai_client = wrap_openai(llm)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m openai_client = \u001b[43mwrap_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m nest_asyncio.apply()\n\u001b[32m     24\u001b[39m retriever = get_vector_db_retriever()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/langchain_obs/intro-to-langsmith-main/.venv/lib/python3.12/site-packages/langsmith/wrappers/_openai.py:437\u001b[39m, in \u001b[36mwrap_openai\u001b[39m\u001b[34m(client, tracing_extra, chat_name, completions_name)\u001b[39m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    435\u001b[39m \u001b[38;5;66;03m# First wrap the create methods - these handle non-streaming cases\u001b[39;00m\n\u001b[32m    436\u001b[39m client.chat.completions.create = _get_wrapper(  \u001b[38;5;66;03m# type: ignore[method-assign]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m437\u001b[39m     \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m.completions.create,\n\u001b[32m    438\u001b[39m     chat_name,\n\u001b[32m    439\u001b[39m     _reduce_chat,\n\u001b[32m    440\u001b[39m     tracing_extra=tracing_extra,\n\u001b[32m    441\u001b[39m     invocation_params_fn=functools.partial(\n\u001b[32m    442\u001b[39m         _infer_invocation_params, \u001b[33m\"\u001b[39m\u001b[33mchat\u001b[39m\u001b[33m\"\u001b[39m, ls_provider\n\u001b[32m    443\u001b[39m     ),\n\u001b[32m    444\u001b[39m     process_outputs=_process_chat_completion,\n\u001b[32m    445\u001b[39m )\n\u001b[32m    447\u001b[39m client.completions.create = _get_wrapper(  \u001b[38;5;66;03m# type: ignore[method-assign]\u001b[39;00m\n\u001b[32m    448\u001b[39m     client.completions.create,\n\u001b[32m    449\u001b[39m     completions_name,\n\u001b[32m   (...)\u001b[39m\u001b[32m    454\u001b[39m     ),\n\u001b[32m    455\u001b[39m )\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# Wrap beta.chat.completions.parse if it exists\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Completions' object has no attribute 'chat'"
     ]
    }
   ],
   "source": [
    "# TODO: Import wrap_openai\n",
    "from langsmith.wrappers import wrap_openai\n",
    "import openai\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "MODEL_PROVIDER = \"openai\"\n",
    "MODEL_NAME = \"gpt-4o-mini\"\n",
    "APP_VERSION = 1.0\n",
    "RAG_SYSTEM_PROMPT = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Use three sentences maximum and keep the answer concise.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: Wrap the OpenAI Client\n",
    "#openai_client = wrap_openai(openai.Client())\n",
    "#openai_client = wrap_openai(llm)\n",
    "openai_client = wrap_openai(llm.client)\n",
    "\n",
    "\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def generate_response(question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": RAG_SYSTEM_PROMPT\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    # TODO: We don't need to use @traceable on a nested function call anymore,\n",
    "    # wrap_openai takes care of this for us\n",
    "    return call_openai(messages)\n",
    "\n",
    "#@traceable\n",
    "def call_openai(\n",
    "    messages: List[dict],\n",
    ") -> str:\n",
    "    return openai_client.invoke(messages)\n",
    "    #     model=MODEL_NAME,\n",
    "    #     messages=messages,\n",
    "    # )\n",
    "\n",
    "@traceable(run_type=\"chain\")\n",
    "def langsmith_rag_with_wrap_openai(question: str):\n",
    "    documents = retrieve_documents(question)\n",
    "    response = generate_response(question, documents)\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with `wrap_openai`, you need to first set the `LANGSMITH_TRACING` environment variable to 'true' and provide your LangSmith API key using `LANGSMITH_API_KEY`. Then, wrap your OpenAI client using `langsmith.wrappers.wrap_openai` when creating the client instance. This setup allows automatic logging of traces without requiring manual function wrapping or decorators.\n"
     ]
    }
   ],
   "source": [
    "question = \"How do I trace with wrap_openai?\"\n",
    "ai_answer = langsmith_rag_with_wrap_openai(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wrapped OpenAI client accepts all the same langsmith_extra parameters as @traceable decorated functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-CUxlGePMvStziqk9AF6SMlFMwvoH6', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content=\"The color of the sky is typically blue during the day due to the scattering of sunlight by the Earth's atmosphere. This phenomenon, known as Rayleigh scattering, causes shorter blue wavelengths of light to be scattered more than the longer red wavelengths. However, the sky can also appear in various colors at different times, such as orange, pink, or red during sunrise and sunset, as well as gray when overcast or during storms. At night, the sky can appear black or dark blue, dotted with stars.\", refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1761495598, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_560af6e559', usage=CompletionUsage(completion_tokens=101, prompt_tokens=13, total_tokens=114, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What color is the sky?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "openai_client.chat.completions.create(\n",
    "    model=MODEL_NAME,\n",
    "    messages=messages,\n",
    "    langsmith_extra={\"metadata\": {\"foo\": \"bar\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Advanced] RunTree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another, more explicit way to log traces to LangSmith is via the RunTree API. This API allows you more control over your tracing - you can manually create runs and children runs to assemble your trace. You still need to set your `LANGSMITH_API_KEY`, but `LANGSMITH_TRACING` is not necessary for this method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![AWTT](../../images/alternative_ways_to_trace_4.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"langsmith-academy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "# I have my env variables defined in a .env file\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and set `LANGSMITH_TRACING` to false, as we are using RunTree to manually create runs in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "\n",
    "from langsmith import utils\n",
    "utils.tracing_is_enabled() # This should return false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have rewritten our RAG application, except this time we pass a RunTree argument through our function calls, and create child runs at each layer. This gives our RunTree the same hierarchy that we were automatically able to establish with @traceable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import RunTree\n",
    "from openai import OpenAI\n",
    "from typing import List\n",
    "import nest_asyncio\n",
    "from utils import get_vector_db_retriever\n",
    "\n",
    "openai_client = OpenAI()\n",
    "nest_asyncio.apply()\n",
    "retriever = get_vector_db_retriever()\n",
    "\n",
    "def retrieve_documents(parent_run: RunTree, question: str):\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Retrieve Documents\",\n",
    "        run_type=\"retriever\",\n",
    "        inputs={\"question\": question},\n",
    "    )\n",
    "    documents = retriever.invoke(question)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"documents\": documents})\n",
    "    child_run.post()\n",
    "    return documents\n",
    "\n",
    "def generate_response(parent_run: RunTree, question: str, documents):\n",
    "    formatted_docs = \"\\n\\n\".join(doc.page_content for doc in documents)\n",
    "    rag_system_prompt = \"\"\"You are an assistant for question-answering tasks. \n",
    "    Use the following pieces of retrieved context to answer the latest question in the conversation. \n",
    "    If you don't know the answer, just say that you don't know. \n",
    "    Use three sentences maximum and keep the answer concise.\n",
    "    \"\"\"\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"Generate Response\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question, \"documents\": documents},\n",
    "    )\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": rag_system_prompt\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Context: {formatted_docs} \\n\\n Question: {question}\"\n",
    "        }\n",
    "    ]\n",
    "    openai_response = call_openai(child_run, messages)\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"openai_response\": openai_response})\n",
    "    child_run.post()\n",
    "    return openai_response\n",
    "\n",
    "def call_openai(\n",
    "    parent_run: RunTree, messages: List[dict], model: str = \"gpt-4o-mini\", temperature: float = 0.0\n",
    ") -> str:\n",
    "    # Create a child run\n",
    "    child_run = parent_run.create_child(\n",
    "        name=\"OpenAI Call\",\n",
    "        run_type=\"llm\",\n",
    "        inputs={\"messages\": messages},\n",
    "    )\n",
    "    openai_response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    # Post the output of our child run\n",
    "    child_run.end(outputs={\"openai_response\": openai_response})\n",
    "    child_run.post()\n",
    "    return openai_response\n",
    "\n",
    "def langsmith_rag(question: str):\n",
    "    # Create a root RunTree\n",
    "    root_run_tree = RunTree(\n",
    "        name=\"Chat Pipeline\",\n",
    "        run_type=\"chain\",\n",
    "        inputs={\"question\": question}\n",
    "    )\n",
    "\n",
    "    # Pass our RunTree into the nested function calls\n",
    "    documents = retrieve_documents(root_run_tree, question)\n",
    "    response = generate_response(root_run_tree, question, documents)\n",
    "    output = response.choices[0].message.content\n",
    "\n",
    "    # Post our final output\n",
    "    root_run_tree.end(outputs={\"generation\": output})\n",
    "    root_run_tree.post()\n",
    "    return output\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To trace with RunTree, you can use the `RunTree.fromHeaders(req.headers)` method to create a run tree from the request headers. Then, utilize the `withRunTree(runTree, () => server(req.body))` helper to ensure that the run tree is propagated within traceable invocations. This approach allows you to log the trace automatically while processing requests in your application.\n"
     ]
    }
   ],
   "source": [
    "question = \"How can I trace with RunTree?\"\n",
    "ai_answer = langsmith_rag(question)\n",
    "print(ai_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
